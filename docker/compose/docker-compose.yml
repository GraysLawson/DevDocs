services:
  frontend:
    build:
      context: .
      dockerfile: docker/dockerfiles/Dockerfile.frontend
    container_name: devdocs-frontend
    ports:
      - "3001:3001"
    environment:
      - BACKEND_URL=http://backend:24125
      - NEXT_PUBLIC_BACKEND_URL=http://backend:24125
      - MCP_HOST=mcp
    depends_on:
      - backend
    networks:
      - devdocs-network
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: docker/dockerfiles/Dockerfile.backend
    container_name: devdocs-backend
    ports:
      - "24125:24125"
    volumes:
      - ./storage:/app/storage
      - ./logs:/app/logs
      - ./crawl_results:/app/crawl_results
    environment:
      - MCP_HOST=mcp
      - CRAWL4AI_URL=http://crawl4ai:11235
      - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-devdocs-demo-key}
    depends_on:
      - crawl4ai
      - mcp
    networks:
      - devdocs-network
    restart: unless-stopped

  mcp:
    build:
      context: .
      dockerfile: docker/dockerfiles/Dockerfile.mcp
    container_name: devdocs-mcp
    volumes:
      - ./storage/markdown:/app/storage/markdown
      - ./logs:/app/logs
    networks:
      - devdocs-network
    stdin_open: true  # Keep stdin open
    tty: true  # Allocate a pseudo-TTY
    restart: unless-stopped

  crawl4ai:
    # IMPORTANT: Make sure you are using an image tag that supports GPU
    # 'gpu-amd64' looks correct based on the name, assuming it exists and is built with CUDA support.
    image: unclecode/crawl4ai:gpu-amd64
    container_name: devdocs-crawl4ai
    ports:
      - "11235:11235"
    environment:
      - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-devdocs-demo-key}
      - MAX_CONCURRENT_TASKS=5 # You might adjust this based on your GPU's capacity
      - DISABLE_AUTH=false
      # Add any GPU-specific environment variables if required by crawl4ai
    volumes:
      - /dev/shm:/dev/shm # Often useful for GPU workloads
      - ./crawl_results:/app/crawl_results
    networks:
      - devdocs-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G # Keep existing memory limits
        reservations:
          memory: 1G # Keep existing memory reservations
          devices:   # <-- Add this 'devices' section under reservations
            - driver: nvidia
              # --- CHOOSE ONE of the following ways to specify GPUs ---
              # Option 1: Assign GPU by Index (most common for single GPU)
              device_ids: ['0']

              # Option 2: Assign a specific number of GPUs (e.g., 1)
              # count: 1

              # Option 3: Assign all available GPUs
              # count: all

              # Option 4: Assign GPU by UUID (find with nvidia-smi -L on host)
              # device_ids: ['GPU-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx']
              # ---------------------------------------------------------
              capabilities: [gpu] # <-- Essential: Request the 'gpu' capability

networks:
  devdocs-network:
    driver: bridge
